base_model: NousResearch/Llama-2-7b-hf
base_model_config: NousResearch/Llama-2-7b-hf
bf16: true
dataset_prepared_path: last_run_prepared
# local
datasets:
  - path: closed_qa_instruct.jsonl
    type: alpaca 
eval_steps: 200
flash_attention: true
gradient_accumulation_steps: 2
gradient_checkpointing: true
learning_rate: 2.0e-05
logging_steps: 1
micro_batch_size: 4
num_epochs: 1
output_dir: /workspace/axolotl/closed_qa
sequence_len: 1024
val_set_size: 0.01
wandb_entity: null
wandb_project: null
