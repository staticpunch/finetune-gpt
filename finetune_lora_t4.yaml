model_name_or_path: /kaggle/input/llama-2/pytorch/7b-hf/1 
train_file: /kaggle/working/patent.jsonl 
bf16: False 
output_dir: /kaggle/working/llama-qlora 
num_train_epochs: 1 
per_device_train_batch_size: 1 
gradient_accumulation_steps: 16
evaluation_strategy: "no" 
save_strategy: "steps" 
save_steps: 500 
learning_rate: 0.0001
warmup_ratio:  0.03 
lr_scheduler_type: "constant" 
max_grad_norm:  0.3 
logging_steps: 10 
do_train: True 
lora_rank: 64 
lora_alpha: 16 
lora_dropout: 0.05 
lora_target_modules: ["from_mapping"] 
dataset_concatenation: True 
max_seq_length: 1024 
low_cpu_mem_usage: True 
train_on_inputs: False 
full_finetune: False
